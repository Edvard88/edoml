{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#Отключим предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Подключаем графические модули:\n",
    "# будем отображать графики прямо в jupyter'e\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# будем отображать графики прямо в jupyter'e\n",
    "%pylab inline\n",
    "\n",
    "#графики в svg выглядят более четкими\n",
    "#%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "#увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "#Подключаем модули для работы с ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Для моделей\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#from sklearn.learning_curve import validation_curve\n",
    "#from sklearn.learning_curve import learning_curve\n",
    "from sklearn.metrics import accuracy_score ,roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#from sklearn.learning_curve import validation_curve\n",
    "#from sklearn.learning_curve import learning_curve\n",
    "from sklearn.metrics import accuracy_score ,roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from itertools import product\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.DataFrame(X[self.columns])\n",
    "    \n",
    "\n",
    "    \n",
    "class ModifiedSimpleImputer(SimpleImputer):\n",
    "    \n",
    "#     def fit_transform(self, X,y=None,  **fit_params):\n",
    "#         self.fit(X,y,  **fit_params)\n",
    "#         return self.transform(X)\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.DataFrame(super().transform(X))\n",
    "    \n",
    "\n",
    "class ModifiedFeatureUnion(FeatureUnion):\n",
    "    \n",
    "#     def fit_transform(self, X,y=None,  **fit_params):\n",
    "#         self.fit(X,y,  **fit_params)\n",
    "#         return self.transform(X)\n",
    "    def merge_dataframes_by_column(self, X):\n",
    "        return pd.concat(X, axis=\"columns\", copy=False)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #X = self.merge_dataframes_by_column(X)\n",
    "        return pd.DataFrame(super().transform(X))\n",
    "    \n",
    "    \n",
    "class MyLEncoder():\n",
    "    \n",
    "    def transform(self, X, **fit_params):\n",
    "        enc = LabelEncoder()\n",
    "        enc_data = []\n",
    "        for i in list(X.columns):\n",
    "            X[i] = X[i].astype(str)\n",
    "            encc = enc.fit(X[i])\n",
    "            enc_data.append(encc.transform(X[i]))\n",
    "        return np.asarray(enc_data).T\n",
    "    \n",
    "    def fit_transform(self, X,y=None,  **fit_params):\n",
    "        self.fit(X,y,  **fit_params)\n",
    "        return self.transform(X)\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        return self "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAutoMlEstimator:\n",
    "    \"\"\"Base class for all estimators in scikit-learn.\n",
    "    Notes\n",
    "    -----\n",
    "    All estimators should specify all the parameters that can be set\n",
    "    at the class level in their ``__init__`` as explicit keyword\n",
    "    arguments (no ``*args`` or ``**kwargs``).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, X_train, X_test, y_train, y_test, target_col, reports_path='/reports'):\n",
    "        self.df = df\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.target_col = target_col        \n",
    "        self.reports_path = reports_path\n",
    "\n",
    "        \n",
    "    @classmethod\n",
    "    def get_params_combination(self, param_grid):\n",
    "        iterator = product(*[v if isinstance(v, Iterable) else [v] for v in param_grid.values()])\n",
    "        return [dict(zip(param_grid.keys(), values)) for values in iterator]\n",
    "    \n",
    "    \n",
    "    def get_report_about_features(self,df, reports_path='/reports'):\n",
    "        \n",
    "        ### Формируем отчет по фичам DF\n",
    "        dict_null = df.isnull().sum().to_dict()\n",
    "        dict_type = df.dtypes.to_dict()\n",
    "\n",
    "        dict_unique = dict()\n",
    "        for column in df.columns:\n",
    "            dict_unique[column] = len(df[column].unique())\n",
    "\n",
    "        report_df = df.describe(include='all').T.rename_axis('features').reset_index()\n",
    "        report_df['feat_count_null'] = report_df['features'].map(dict_null)\n",
    "        report_df['feat_type'] = report_df['features'].map(dict_type)\n",
    "        report_df['unique'] = report_df['features'].map(dict_unique)\n",
    "        \n",
    "        report_df.to_csv('..'+ reports_path + '/features_report.csv', index=False)   \n",
    "        \n",
    "        print(\"Отчет по статистикам по фичам сформирован\")\n",
    "        return report_df\n",
    "\n",
    "    \n",
    "    \n",
    "    def optimize_types(self, df, inplace=False):\n",
    "    \n",
    "        np_types = [np.int8 ,np.int16 ,np.int32, np.int64,\n",
    "                np.uint8 ,np.uint16, np.uint32, np.uint64]\n",
    "\n",
    "        np_types = [np_type.__name__ for np_type in np_types]\n",
    "        type_df = pd.DataFrame(data=np_types, columns=['class_type'])\n",
    "\n",
    "        type_df['min_value'] = type_df['class_type'].apply(lambda row: np.iinfo(row).min)\n",
    "        type_df['max_value'] = type_df['class_type'].apply(lambda row: np.iinfo(row).max)\n",
    "        type_df['range'] = type_df['max_value'] - type_df['min_value']\n",
    "        type_df.sort_values(by='range', inplace=True)\n",
    "\n",
    "\n",
    "        for col in df.loc[:, df.dtypes <= np.integer]:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            temp = type_df[(type_df['min_value'] <= col_min) & (type_df['max_value'] >= col_max)]\n",
    "            optimized_class = temp.loc[temp['range'].idxmin(), 'class_type']\n",
    "            print(\"Название колонки : {} Минимальное значение : {} Максимальное значение : {} Можно оптимизировать к  : {}\"\\\n",
    "                  .format(col, col_min, col_max, optimized_class))\n",
    "\n",
    "            if inplace == 'True':\n",
    "                df[col] = df[col].astype(optimized_class)\n",
    "\n",
    "        #df.info()\n",
    "\n",
    "        #return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoMlClassification(BaseAutoMlEstimator):    \n",
    "    \n",
    "    def __init__(self, df, X_train, X_test, y_train, y_test, target_col, reports_path='/reports'):\n",
    "        super().__init__(df, X_train, X_test, y_train, y_test, target_col, reports_path='/reports')\n",
    "        \n",
    "    \n",
    "    def get_report_about_target(self, df, target_col ,reports_path='/reports'):\n",
    "        \n",
    "        ### Формируем отчет по таргету DF\n",
    "        \n",
    "        target_report = df.groupby(target_col).count().reset_index()\n",
    "\n",
    "        dict_count_traget = df[target_col].value_counts().to_dict()\n",
    "        dict_count_traget_norm = df[target_col].value_counts(normalize=True).to_dict()\n",
    "\n",
    "        target_report['count'] = target_report[target_col].map(dict_count_traget)\n",
    "        target_report['count_norm'] = target_report[target_col].map(dict_count_traget_norm)\n",
    "        \n",
    "        target_report.to_csv('..'+ reports_path + '/target_report.csv', index=False)    \n",
    "        \n",
    "        print(\"Отчет по статистикам по таргету сформирован\")\n",
    "        return target_report\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit_report(self, X_train, X_test, y_train, y_test, reports_path='/reports'):\n",
    "        \n",
    "        digits_features = X_train.select_dtypes(include=['number']).columns.values.tolist()\n",
    "        cat_features = X_train.select_dtypes(include=['object', ]).columns.values.tolist()\n",
    "        \n",
    "        classifiers = [LogisticRegression,\n",
    "               #KNeighborsClassifier,\n",
    "               #GradientBoostingClassifier(), \n",
    "               RandomForestClassifier] \n",
    "        #               SVC()] # \n",
    "\n",
    "        classifiers_name = ['LogisticRegression',\n",
    "\n",
    "                            #'KNeighborsClassifier',\n",
    "                            #'GradientBoostingClassifier', \n",
    "                            'RandomForestClassifier'] \n",
    "        #                    'SVC']\n",
    "\n",
    "\n",
    "        # Настройка параметров выбранных алгоритмов с помощью GridSearchCV \n",
    "        n_folds = 5\n",
    "        scores = []\n",
    "        fits = []\n",
    "        logistic_params = {'penalty': ('l1', 'l2'),\n",
    "                           'C': (.01,5)}\n",
    "\n",
    "        knn_params = {'n_neighbors': list(range(3, 6, 2))}\n",
    "\n",
    "\n",
    "        gbm_params = {'n_estimators': [100, 300, 500],\n",
    "                      'learning_rate':(0.1, 0.5, 1),\n",
    "                      'max_depth': list(range(3, 6)), \n",
    "                      'min_samples_leaf': list(range(10, 31, 10))}\n",
    "\n",
    "\n",
    "\n",
    "        forest_params = {'n_estimators': [10, 30],\n",
    "                         'criterion': ('gini', 'entropy')}\n",
    "\n",
    "        #svm_param = {'kernel' : ('linear', 'rbf'), 'C': (.5, 1, 2)} - очень долго считал\n",
    "        #params = [logistic_params, knn_params, gbm_params, forest_params]\n",
    "\n",
    "        params = [logistic_params, forest_params]        \n",
    "\n",
    "\n",
    "\n",
    "        ############# Заполнение NaN #############\n",
    "        imputer_numeric_list = [ModifiedSimpleImputer]\n",
    "        imputer_numeric_name = ['ModifiedSimpleImputer'] \n",
    "\n",
    "        simpleimputer_params_numeric = {'fill_value': [0, -1],\n",
    "                                        'strategy': ['constant']}\n",
    "        params_numeric_list = [simpleimputer_params_numeric]        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        imputer_cat_list = [ModifiedSimpleImputer]\n",
    "        imputer_cat_name = ['ModifiedSimpleImputer'] \n",
    "\n",
    "        simpleimputer_params_cat = {'strategy': ['constant'],\n",
    "                                   'fill_value': ['missing']}\n",
    "        params_cat_list = [simpleimputer_params_cat]        \n",
    "\n",
    "        ############# Заполнение NaN #############\n",
    "\n",
    "\n",
    "\n",
    "        ############# Scaler ################\n",
    "\n",
    "        scaler_list = [MinMaxScaler]\n",
    "        scaler_name_list = ['MinMaxScaler'] \n",
    "\n",
    "        scaler_params = {'feature_range': [(0,1) , (2,3)]}\n",
    "        params_scaler_list = [scaler_params]        \n",
    "\n",
    "\n",
    "        #####################################\n",
    "\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        df1 = pd.DataFrame()\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=3, random_state=0)\n",
    "\n",
    "\n",
    "        # for i , each_imputer_cat in enumerate(imputer_cat_list):\n",
    "        #     imputer_cat = each_imputer_cat\n",
    "        #     imputer_cat_name = imputer_cat_name[i]\n",
    "        #     imputer_cat_params = params_cat_list[i]\n",
    "        #     print(\"imputer_cat_name\", imputer_cat_name)    \n",
    "        #     for tmp_imputer_cat_params in get_params_combination(imputer_cat_params):\n",
    "        #         print(\"Параметры: \", tmp_imputer_cat_params)\n",
    "\n",
    "\n",
    "        for i , each_imputer_numeric in enumerate(imputer_numeric_list):\n",
    "            imputer_numeric = each_imputer_numeric\n",
    "            imputer_numeric_name = imputer_numeric_name[i]\n",
    "            imputer_numeric_params = params_numeric_list[i]\n",
    "            print(\"imputer_numeric_name fill_na\", imputer_numeric_name)    \n",
    "            for current_imputer_numeric_params in self.get_params_combination(imputer_numeric_params):\n",
    "                print('\\n', \"Параметры: \", current_imputer_numeric_params)\n",
    "\n",
    "\n",
    "                for i , each_scaler in enumerate(scaler_list):\n",
    "                    scaler = each_scaler\n",
    "                    scaler_name = scaler_name_list[i]\n",
    "                    scaler_params = params_scaler_list[i]\n",
    "                    print(\"scaler_name\", scaler_name,)    \n",
    "                    for current_scaler_param in self.get_params_combination(scaler_params):\n",
    "                        print('\\n', \"Параметры scaler: \", current_scaler_param, '\\n')\n",
    "\n",
    "                        for i , each_classifier in enumerate(classifiers):\n",
    "                            clf = each_classifier\n",
    "                            clf_params = params[i]\n",
    "                            clf_classifiers_name = classifiers_name[i]\n",
    "                            print(\"classifiers_name\", clf_classifiers_name,)\n",
    "\n",
    "                            for tmp_params in self.get_params_combination(clf_params):\n",
    "                                print(\"Параметры classifiers: \", tmp_params)\n",
    "                                skf_index = skf.split(X_train, y_train)\n",
    "                                for fold, (train_idx, test_idx) in enumerate(skf_index):\n",
    "                                    print(\"Размер тренировочного / тестового датасета: \", len(train_idx), len(test_idx))\n",
    "\n",
    "                                    # Формируем тренеровочный и валидационный датасет\n",
    "                                    X_train_tmp, X_test_tmp = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
    "                                    y_train_tmp, y_test_tmp = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "\n",
    "                                    # Получаем модель\n",
    "                                    #tmp_clf = clf(**tmp_params)\n",
    "\n",
    "                                    tmp_clf = Pipeline([\n",
    "                                            # Use FeatureUnion to combine the features\n",
    "                                            ('union', ModifiedFeatureUnion(\n",
    "                                                transformer_list=[\n",
    "                                                     # categorical features\n",
    "                                                    ('categorical', Pipeline([\n",
    "                                                         ('selector', FeatureSelector(columns = cat_features)),\n",
    "                                                         ('imputer', ModifiedSimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                                         ('label_encoding', MyLEncoder())\n",
    "                                                    ])),\n",
    "                                                    # numeric features\n",
    "                                                    ('numeric', Pipeline([\n",
    "                                                         ('selector', FeatureSelector(columns = digits_features)),\n",
    "                                                         ('imputer', imputer_numeric(**current_imputer_numeric_params)),\n",
    "                                                         ('scaler', scaler(**current_scaler_param))\n",
    "                                                    ])),\n",
    "                                                ])),\n",
    "                                            # Use model fit\n",
    "                                            ('model_fitting', clf(**tmp_params)),\n",
    "                                        ])\n",
    "\n",
    "\n",
    "\n",
    "                                    # Замеряем время fit\n",
    "                                    start_time = time.time()\n",
    "                                    pred = tmp_clf.fit(X_train_tmp, y_train_tmp)\n",
    "                                    fit_time = time.time() - start_time\n",
    "\n",
    "\n",
    "                                    # Замеряем время predict\n",
    "                                    start_time = time.time()\n",
    "                                    pred = tmp_clf.predict(X_test_tmp)\n",
    "                                    predict_time = time.time() - start_time\n",
    "\n",
    "                                    clf_tmp_params_string = \", \".join((\"{}={}\".format(*i) for i in tmp_params.items()))\n",
    "                                    scale_tmp_params_string = \", \".join((\"{}={}\".format(*i) for i in current_scaler_param.items()))\n",
    "                                    imputer_numeric_params_string = \", \".join((\"{}={}\".format(*i) for i in current_imputer_numeric_params.items()))\n",
    "\n",
    "\n",
    "                                    data = {'classifier_name' : clf_classifiers_name,\n",
    "                                            'classifier_params' : clf_tmp_params_string,\n",
    "                                            'scaler_name': scaler_name,\n",
    "                                            'scaler_params': scale_tmp_params_string,\n",
    "\n",
    "                                            'imputer_name': imputer_numeric_name,\n",
    "                                            'imputer_params': imputer_numeric_params_string,\n",
    "\n",
    "                                            'fold' : fold,\n",
    "                                            'fit_time' : fit_time, \n",
    "                                            'predict_time' : predict_time,\n",
    "                                            'roc_auc':roc_auc_score(y_test_tmp, pred)\n",
    "\n",
    "                                    }\n",
    "\n",
    "                                    # Расширяем другими параметрами\n",
    "                                    data.update(tmp_params) # параметры классификатора\n",
    "                                    data.update(current_scaler_param) # параметры scale\n",
    "                                    data.update(current_imputer_numeric_params)\n",
    "\n",
    "                                    # Формируем финальный датафрейм\n",
    "                                    df1 = df1.append(data, ignore_index=True)\n",
    "\n",
    "                    \n",
    "        df1.to_csv('..'+ reports_path + '/model_report.csv', index=False)  \n",
    "        print(\"Отчет по модели сформирован\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем данные и разбиваем их на Train и Test\n",
    "\n",
    "df = pd.read_csv('../ml_data/train.csv')\n",
    "target_col = 'Survived'\n",
    "\n",
    "y = df[target_col]\n",
    "X = df.drop([target_col], axis=1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем экземпляр класса\n",
    "\n",
    "#reports_path='/reports'\n",
    "au1 = AutoMlClassification(df, X_train, X_test, y_train, y_test, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчет по статистикам по таргету сформирован\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>count</th>\n",
       "      <th>count_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>424</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>68</td>\n",
       "      <td>549</td>\n",
       "      <td>549</td>\n",
       "      <td>0.616162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>290</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>342</td>\n",
       "      <td>136</td>\n",
       "      <td>340</td>\n",
       "      <td>342</td>\n",
       "      <td>0.383838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  PassengerId  Pclass  Name  Sex  Age  SibSp  Parch  Ticket  Fare  \\\n",
       "0         0          549     549   549  549  424    549    549     549   549   \n",
       "1         1          342     342   342  342  290    342    342     342   342   \n",
       "\n",
       "   Cabin  Embarked  count  count_norm  \n",
       "0     68       549    549    0.616162  \n",
       "1    136       340    342    0.383838  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Формируем отчет по таргету\n",
    "\n",
    "au1.get_report_about_target(df, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Черновик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(func):\n",
    "    def wrapper(self):\n",
    "        func(self)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n",
      "foo\n",
      "foo\n"
     ]
    }
   ],
   "source": [
    "from functools import wraps\n",
    "class Repeater:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, f):\n",
    "        @wraps(f)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for _ in range(self.n):\n",
    "                f(*args, **kwargs)\n",
    "        return wrapper\n",
    "       \n",
    "    \n",
    "@Repeater(3)\n",
    "def foo():\n",
    "    print('foo')\n",
    "\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__test__() missing 2 required positional arguments: 'self' and 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-481247249fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mRepeater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__test__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __test__() missing 2 required positional arguments: 'self' and 'f'"
     ]
    }
   ],
   "source": [
    "@Repeater.__test__()\n",
    "def foo():\n",
    "    print('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def count(func):\n",
    "    def wrapper(self):\n",
    "        TestClass.call_count += 1\n",
    "        func(self)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class TestClass(object):\n",
    "    call_count = 0\n",
    "\n",
    "    @count\n",
    "    def hello(self):\n",
    "        return 'hello'\n",
    "\n",
    "\n",
    "x = TestClass()\n",
    "for i in range(10):\n",
    "    x.hello()\n",
    "\n",
    "print(TestClass.call_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Spark + Teradata\n",
    "\n",
    "data = spark.read.jdbc(\"sbx_datamart.card_auths\")\n",
    "\n",
    "data.filter(col(\"city\") == \"IRKUTSK\").select(\"*\").write...\n",
    "# Failed to perform, kicked by DBA\n",
    "# in option query\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "\n",
    "\n",
    "# 2) Spark SQL part\n",
    "trans = spark.table(\"fraud.events\")\n",
    "cards = spark.table(\"fraud.bank_cards\")\n",
    "\n",
    "\"\"\"\n",
    "trans:\n",
    "    event_id\n",
    "    user_id\n",
    "    trans_date\n",
    "    card_number\n",
    "    event_amount\n",
    "    merchant_name\n",
    "    mcc\n",
    "    \n",
    "\n",
    "cards:\n",
    "    card_number\n",
    "    user_id\n",
    "    office_id\n",
    "    org_office\n",
    "    segment\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "datamart = trans.join(cards, [\"card_number\"])\\\n",
    "    .groupBy(\"user_id\", \"trans_date\")\\\n",
    "    .agg(avg(\"event_amount\").as(\"per_user_avg\"))\n",
    "    .filter(col(\"per_user_avg\") > 123000)\n",
    "\n",
    "# Failed to perform, OutOfMemoryError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")  Множим таблицу  card с дополнительным полем id куда ставим цифры от 1 до 10.  те будет 10 таблицы card и у них в каждой будет одно значение id.  \n",
    "\n",
    "2) Далее добавлемя к trans колонку id и туда рандомно ставим номер от 1 до 10. \n",
    "\n",
    "Далее делаем джойн, благодаря такому моменту получается, что джойн идет равномерно на экзекютеры.\n",
    "\n",
    "Делается пункт 1) с помощью функции exploed\n",
    "\n",
    "Также скошенность здесь по card_number  skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://docs.databricks.com/delta/join-performance/skew-join.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
